{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    # for regular expressions \n",
    "import nltk  # for text manipulation \n",
    "import string \n",
    "import warnings \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train  = pd.read_csv('train_E6oV3lV.csv') \n",
    "test = pd.read_csv('test_tweets_anuFYb8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @user @user @user @user dannyâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr8 !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "2   3      0   \n",
       "3   4      0   \n",
       "4   5      0   \n",
       "5   6      0   \n",
       "6   7      0   \n",
       "7   8      0   \n",
       "8   9      0   \n",
       "9  10      0   \n",
       "\n",
       "                                                                                                                                             tweet  \n",
       "0                                            @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "1                       @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2                                                                                                                              bihday your majesty  \n",
       "3                                                           #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦    \n",
       "4                                                                                                           factsguide: society now    #motivation  \n",
       "5                             [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo    \n",
       "6                                                                        @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦  \n",
       "7  the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl  \n",
       "8                                                          we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦   \n",
       "9                                                                                                @user @user welcome here !  i'm   it's so #gr8 !   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['label'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #cnn calls #michigan middle school 'build the wall' chant '' #tcot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>@user lets fight against  #love #peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>@user hey, white people: you can call people 'white' by @user  #race  #identity #medâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>how the #altright uses  &amp;amp; insecurity to lure men into #whitesupremacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>@user i'm not interested in a #linguistics that doesn't address #race &amp;amp; . racism is about #power. #raciolinguistics bringsâ¦</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label  \\\n",
       "13    14      1   \n",
       "14    15      1   \n",
       "17    18      1   \n",
       "23    24      1   \n",
       "34    35      1   \n",
       "56    57      1   \n",
       "68    69      1   \n",
       "77    78      1   \n",
       "82    83      1   \n",
       "111  112      1   \n",
       "\n",
       "                                                                                                                                 tweet  \n",
       "13                                                          @user #cnn calls #michigan middle school 'build the wall' chant '' #tcot    \n",
       "14                               no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins  \n",
       "17                                                                                                              retweet if you agree!   \n",
       "23                                                                                     @user @user lumpy says i am a . prove it lumpy.  \n",
       "34                            it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia   \n",
       "56                                                                                             @user lets fight against  #love #peace   \n",
       "68                      ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness    \n",
       "77                                             @user hey, white people: you can call people 'white' by @user  #race  #identity #medâ¦  \n",
       "82                                                       how the #altright uses  &amp; insecurity to lure men into #whitesupremacy      \n",
       "111  @user i'm not interested in a #linguistics that doesn't address #race &amp; . racism is about #power. #raciolinguistics bringsâ¦  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['label'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31962, 3), (17197, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_train = train['tweet'].str.len() \n",
    "length_test = test['tweet'].str.len() \n",
    "plt.hist(length_train, bins=20, label=\"train_tweets\") \n",
    "plt.hist(length_test, bins=20, label=\"test_tweets\") \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noren/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49159, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine the Train and Test Data for PrePocessing\n",
    "combi = train.append(test, ignore_index=True) \n",
    "combi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Given below is a user-defined function to remove unwanted text patterns from the tweets.\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1    0.0   \n",
       "1   2    0.0   \n",
       "2   3    0.0   \n",
       "3   4    0.0   \n",
       "4   5    0.0   \n",
       "\n",
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                                         tidy_tweet  \n",
       "0                   when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "1    thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2                                                                                               bihday your majesty  \n",
       "3                            #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦    \n",
       "4                                                                            factsguide: society now    #motivation  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Removing Twitter Handles (@user)\n",
    "#\n",
    "#Let’s create a new column tidy_tweet, it will contain the cleaned and processed tweets. Note that we have passed “@[]*” as the pattern to the remove_pattern function. It is actually a regular expression which will pick any word starting with ‘@’.\n",
    "\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction    #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks for #lyft credit i can t use cause they don t offer wheelchair vans in pdx     #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>#model   i love u take with u all the time in ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide  society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>\n",
       "      <td>huge fan fare and big talking before they leave  chaos and pay disputes when they get there  #allshowandnogo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @user @user @user @user dannyâ¦</td>\n",
       "      <td>camping tomorrow        danny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n",
       "      <td>the next school year is the year for exams      can t think about that      #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦</td>\n",
       "      <td>we won    love the land    #allin #cavs #champions #cleveland #clevelandcavaliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr8 !</td>\n",
       "      <td>welcome here    i m   it s so #gr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1    0.0   \n",
       "1   2    0.0   \n",
       "2   3    0.0   \n",
       "3   4    0.0   \n",
       "4   5    0.0   \n",
       "5   6    0.0   \n",
       "6   7    0.0   \n",
       "7   8    0.0   \n",
       "8   9    0.0   \n",
       "9  10    0.0   \n",
       "\n",
       "                                                                                                                                             tweet  \\\n",
       "0                                            @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1                       @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                                              bihday your majesty   \n",
       "3                                                           #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                                           factsguide: society now    #motivation   \n",
       "5                             [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo     \n",
       "6                                                                        @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦   \n",
       "7  the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl   \n",
       "8                                                          we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦    \n",
       "9                                                                                                @user @user welcome here !  i'm   it's so #gr8 !    \n",
       "\n",
       "                                                                                                                                        tidy_tweet  \n",
       "0                                                  when a father is dysfunctional and is so selfish he drags his kids into his dysfunction    #run  \n",
       "1                                   thanks for #lyft credit i can t use cause they don t offer wheelchair vans in pdx     #disapointed #getthanked  \n",
       "2                                                                                                                              bihday your majesty  \n",
       "3                                                           #model   i love u take with u all the time in ur                                        \n",
       "4                                                                                                           factsguide  society now    #motivation  \n",
       "5                                   huge fan fare and big talking before they leave  chaos and pay disputes when they get there  #allshowandnogo    \n",
       "6                                                                                                                 camping tomorrow        danny     \n",
       "7  the next school year is the year for exams      can t think about that      #school #exams   #hate #imagine #actorslife #revolutionschool #girl  \n",
       "8                                                          we won    love the land    #allin #cavs #champions #cleveland #clevelandcavaliers        \n",
       "9                                                                                                            welcome here    i m   it s so #gr      "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Removing Punctuations, Numbers, and Special Characters\n",
    "\n",
    "#Here we will replace everything except characters and hashtags with spaces. The regular expression “[^a-zA-Z#]” means anything except alphabets and ‘#’.\n",
    "\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \") \n",
    "combi.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when father dysfunctional selfish drags kids into dysfunction #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks #lyft credit cause they offer wheelchair vans #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>#model love take with time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1    0.0   \n",
       "1   2    0.0   \n",
       "2   3    0.0   \n",
       "3   4    0.0   \n",
       "4   5    0.0   \n",
       "\n",
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                      tidy_tweet  \n",
       "0             when father dysfunctional selfish drags kids into dysfunction #run  \n",
       "1  thanks #lyft credit cause they offer wheelchair vans #disapointed #getthanked  \n",
       "2                                                            bihday your majesty  \n",
       "3                                                     #model love take with time  \n",
       "4                                                 factsguide society #motivation  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Removing Short Words\n",
    "\n",
    "#We have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 3 or less. For example, terms like “hmm”, “oh” are of very little use. It is better to get rid of them.\n",
    "\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "#Let’s take another look at the first few rows of the combined dataframe.\n",
    "\n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [when, father, dysfunctional, selfish, drags, kids, into, dysfunction, #run]\n",
       "1    [thanks, #lyft, credit, cause, they, offer, wheelchair, vans, #disapointed, #getthanked]\n",
       "2                                                                     [bihday, your, majesty]\n",
       "3                                                            [#model, love, take, with, time]\n",
       "4                                                          [factsguide, society, #motivation]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Text Normalization\n",
    "\n",
    "#Here we will use nltk’s PorterStemmer() function to normalize the tweets. But before that we will have to tokenize the tweets. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.\n",
    "\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \n",
    "tokenized_tweet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can normalize the tokenized tweets.\n",
    "\n",
    "from nltk.stem.porter import * \n",
    "stemmer = PorterStemmer() \n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let’s stitch these tokens back together. It can easily be done using nltk’s MosesDetokenizer function.\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])    \n",
    "combi['tidy_tweet'] = tokenized_tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Story Generation and Visualization from Tweets\n",
    "#\n",
    "#Before we begin exploration, we must think and ask questions related to the data in hand. A few probable questions are as follows:\n",
    "#\n",
    "#What are the most common words in the entire dataset?\n",
    "#What are the most common words in the dataset for negative and positive tweets, respectively?\n",
    "#How many hashtags are there in a tweet?\n",
    "#Which trends are associated with my dataset?\n",
    "#Which trends are associated with either of the sentiments? Are they compatible with the sentiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BAG of Words Features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "import gensim\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') \n",
    "bow = bow_vectorizer.fit_transform(combi['tidy_tweet']) \n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Features\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') \n",
    "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet']) \n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6510632, 7536020)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s train a Word2Vec model on our corpus.\n",
    "\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=200, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 4, # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spaghetti', 0.5827436447143555),\n",
       " ('#avocado', 0.5773341655731201),\n",
       " ('#biall', 0.5761107206344604),\n",
       " ('#cellar', 0.5543804168701172),\n",
       " ('#toast', 0.5485936403274536),\n",
       " ('aladdin', 0.5453238487243652),\n",
       " ('cookout', 0.5403849482536316),\n",
       " ('dess', 0.5347200632095337),\n",
       " ('spinach', 0.5327342748641968),\n",
       " ('#foodcoma', 0.5325911641120911)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"dinner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.5905888080596924),\n",
       " ('melo', 0.5417779088020325),\n",
       " ('tomlin', 0.5402119755744934),\n",
       " ('unfavor', 0.5383583307266235),\n",
       " ('phoni', 0.5363662242889404),\n",
       " ('businessman', 0.5220706462860107),\n",
       " ('unstabl', 0.5215750932693481),\n",
       " ('#delegaterevolt', 0.5211461782455444),\n",
       " ('jibe', 0.5208595395088196),\n",
       " ('irrefut', 0.5188117027282715)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"trump\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5580425 ,  0.01415924,  0.6119647 , -0.07116   ,  0.82325494,\n",
       "        0.40704405, -0.73009443, -0.10448189,  0.92249495,  0.206207  ,\n",
       "        0.24073061, -0.6218729 , -0.32814255, -0.09495009, -0.42761502,\n",
       "       -0.1015223 ,  0.5481348 ,  0.12097079, -0.80009395,  0.01961496,\n",
       "        1.0176357 ,  0.29980618, -0.28796032, -0.4294438 ,  0.29889336,\n",
       "       -0.0364139 , -0.33634716, -0.14371936,  0.35859343, -0.5313806 ,\n",
       "       -0.5437804 , -0.9368403 ,  0.4270831 ,  0.07391904,  0.11772979,\n",
       "       -0.66865593, -0.25149828,  0.3286533 ,  0.41323155, -0.26510322,\n",
       "       -0.1369952 , -0.47353202,  0.28347266,  0.01820521, -0.5661762 ,\n",
       "        0.32983708, -0.81525713, -0.123602  ,  0.27550614, -0.04431631,\n",
       "       -0.15525898, -0.33099192,  0.69276   ,  0.49291405,  0.04373464,\n",
       "       -0.45318738,  0.06032811,  0.7372708 , -0.20010655,  0.20881864,\n",
       "        0.04621487,  0.2662759 , -0.32290766, -0.0030121 , -0.7011091 ,\n",
       "        0.26005533, -0.36391214, -0.19865783,  0.45734134,  0.05871556,\n",
       "       -0.2951552 ,  0.3411663 , -0.8764602 ,  0.7175176 , -0.13330062,\n",
       "       -0.6170691 , -0.14600614, -0.49827522, -0.63058436,  0.18334176,\n",
       "       -0.18391064, -0.15812247, -0.5893201 ,  0.23605347, -0.37999687,\n",
       "        0.02817929,  0.03108555,  0.1861056 , -0.09642472, -0.44391915,\n",
       "        0.515753  ,  0.39073426, -0.31807217, -0.25486043, -0.07990468,\n",
       "       -0.8264725 , -0.5039472 , -1.3078048 ,  0.15745404, -0.55319095,\n",
       "        0.19719073, -0.32487795,  0.7179582 ,  0.21506616, -0.7853402 ,\n",
       "        0.42793682, -0.3657158 , -0.7068838 , -0.46281275, -0.50547945,\n",
       "        0.21044257,  0.24642725,  0.8643794 , -0.6380443 , -0.41125366,\n",
       "       -0.31763443,  1.0644965 , -0.54827905,  0.4709458 , -0.10992017,\n",
       "       -0.38608274, -0.00999748,  0.42782375, -0.6497793 ,  0.2835571 ,\n",
       "        0.5898653 , -0.13819012, -0.30593944, -0.39824745,  0.15548201,\n",
       "       -0.73742545,  0.6538398 , -0.07553285, -0.12411185, -0.2758204 ,\n",
       "        0.09519506, -0.19814605, -0.11542172,  0.217973  , -0.37459913,\n",
       "        0.32380566, -0.10087241,  0.56971157,  0.5782679 , -0.23478206,\n",
       "        0.6711548 ,  0.4500439 , -0.7077469 , -0.11069191,  0.8161317 ,\n",
       "       -0.11381236,  0.4843917 ,  0.2230293 ,  0.34472063,  0.15249923,\n",
       "        0.2122335 ,  0.4116404 ,  0.45599273,  0.00376007,  0.17506175,\n",
       "        0.07462798,  0.24881907, -0.11035622, -0.08784626,  0.24046011,\n",
       "        0.23708555,  0.10041105,  0.31692302, -0.13052666, -0.1260331 ,\n",
       "        0.39154175,  0.20685042, -0.39244744, -0.42199868, -0.23445256,\n",
       "        0.06486131, -0.35607734, -0.28962025, -0.43850264,  0.68600327,\n",
       "       -0.26587653,  0.03527354, -0.6919477 ,  0.54117936, -0.31754825,\n",
       "       -0.1918837 , -0.7559375 ,  0.00949717,  0.00900236, -0.5778732 ,\n",
       "       -0.5165861 ,  0.39403638,  0.6948399 , -0.6771373 , -0.66332936,\n",
       "       -0.43708476,  0.3927048 , -0.18208449,  0.2873743 , -0.74753034],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v['food']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_w2v['food'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making Vectors from Tweets\n",
    "#We will use the below function to create a vector for each tweet by taking the average of the vectors of the words present in the tweet.\n",
    "\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "#Preparing word2vec feature set…\n",
    "\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays) \n",
    "wordvec_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Doc2Vec Embedding\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas(desc=\"progress-bar\") \n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "#To implement doc2vec, we have to labelise or tag each tokenised tweet with unique IDs. We can do so by using Gensim’s LabeledSentence() function.\n",
    "\n",
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "\n",
    "labeled_tweets = add_label(tokenized_tweet) # label all the tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['when', 'father', 'dysfunct', 'selfish', 'drag', 'kid', 'into', 'dysfunct', '#run'], tags=['tweet_0']),\n",
       " LabeledSentence(words=['thank', '#lyft', 'credit', 'caus', 'they', 'offer', 'wheelchair', 'van', '#disapoint', '#getthank'], tags=['tweet_1']),\n",
       " LabeledSentence(words=['bihday', 'your', 'majesti'], tags=['tweet_2']),\n",
       " LabeledSentence(words=['#model', 'love', 'take', 'with', 'time'], tags=['tweet_3']),\n",
       " LabeledSentence(words=['factsguid', 'societi', '#motiv'], tags=['tweet_4']),\n",
       " LabeledSentence(words=['huge', 'fare', 'talk', 'befor', 'they', 'leav', 'chao', 'disput', 'when', 'they', 'there', '#allshowandnogo'], tags=['tweet_5'])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49159/49159 [00:00<00:00, 989921.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#Now let’s train a doc2vec model.\n",
    "model_d2v = gensim.models.Doc2Vec(dm=1,  # dm = 1 for ‘distributed memory’ model dm_mean=1, # dm = 1 for using mean of the context word vectors                                  size=200, # no. of desired features                                  \n",
    "vector_size=200, # no. of desired features \n",
    "window=5, # width of the context window                                  \n",
    "negative=7, # if > 0 then negative sampling will be used min_count=5, # Ignores all words with total frequency lower than 2.                                  \n",
    "workers=4, # no. of cores                                  \n",
    "alpha=0.1, # learning rate                                  \n",
    "seed = 23) \n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
    "model_d2v.train(labeled_tweets, total_examples= len(combi['tidy_tweet']), epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 200)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preparing doc2vec Feature Set\n",
    "\n",
    "docvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
    "for i in range(len(combi)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))    \n",
    "\n",
    "docvec_df = pd.DataFrame(docvec_arrays) \n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5307820299500832"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#On Bag of Words\n",
    "# Extracting train and test BoW features \n",
    "train_bow = bow[:31962,:] \n",
    "test_bow = bow[31962:,:] \n",
    "# splitting data into training and validation set \n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'],                                                            random_state=42,                                                           test_size=0.3)\n",
    "lreg = LogisticRegression() \n",
    "# training the model \n",
    "lreg.fit(xtrain_bow, ytrain) \n",
    "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set \n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int) # calculating f1 score for the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let’s make predictions for the test dataset and create a submission file.\n",
    "\n",
    "test_pred = lreg.predict_proba(test_bow) \n",
    "test_pred_int = test_pred[:,1] >= 0.3 \n",
    "test_pred_int = test_pred_int.astype(np.int) \n",
    "test['label'] = test_pred_int \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5446507515473032"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Features\n",
    "\n",
    "#We’ll follow the same steps as above, but now for the TF-IDF feature set.\n",
    "\n",
    "train_tfidf = tfidf[:31962,:] \n",
    "test_tfidf = tfidf[31962:,:] \n",
    "xtrain_tfidf = train_tfidf[ytrain.index] \n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "lreg.fit(xtrain_tfidf, ytrain) \n",
    "prediction = lreg.predict_proba(xvalid_tfidf) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int) # calculating f1 score for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6177549523110785"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec Features\n",
    "\n",
    "train_w2v = wordvec_df.iloc[:31962,:] \n",
    "test_w2v = wordvec_df.iloc[31962:,:] \n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:] \n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n",
    "lreg.fit(xtrain_w2v, ytrain) \n",
    "prediction = lreg.predict_proba(xvalid_w2v) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.385499557913351"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Doc2Vec Features\n",
    "\n",
    "train_d2v = docvec_df.iloc[:31962,:] \n",
    "test_d2v = docvec_df.iloc[31962:,:] \n",
    "xtrain_d2v = train_d2v.iloc[ytrain.index,:] \n",
    "xvalid_d2v = train_d2v.iloc[yvalid.index,:]\n",
    "lreg.fit(xtrain_d2v, ytrain) \n",
    "prediction = lreg.predict_proba(xvalid_d2v) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22403733955659275"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Support Vector Machine (SVM)\n",
    "from sklearn import svm\n",
    "\n",
    "#Bag-of-Words Features\n",
    "\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain) \n",
    "prediction = svc.predict_proba(xvalid_bow) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)\n",
    "#0.508\n",
    "\n",
    "#Again let’s make predictions for the test dataset and create another submission file.\n",
    "\n",
    "test_pred = svc.predict_proba(test_bow) \n",
    "test_pred_int = test_pred[:,1] >= 0.3 \n",
    "test_pred_int = test_pred_int.astype(np.int) \n",
    "test['label'] = test_pred_int \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_svm_bow.csv', index=False)\n",
    "\n",
    "\n",
    "#TF-IDF Features\n",
    "\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_tfidf, ytrain) \n",
    "prediction = svc.predict_proba(xvalid_tfidf) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)\n",
    "#0.51\n",
    "\n",
    "#Word2Vec Features\n",
    "\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_w2v, ytrain) \n",
    "prediction = svc.predict_proba(xvalid_w2v) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)\n",
    "#0.614\n",
    "\n",
    "#Doc2Vec Features\n",
    "\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_d2v, ytrain) \n",
    "prediction = svc.predict_proba(xvalid_d2v) \n",
    "prediction_int = prediction[:,1] >= 0.3 \n",
    "prediction_int = prediction_int.astype(np.int) \n",
    "f1_score(yvalid, prediction_int)\n",
    "#0.203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Bag-of-Words Features\n",
    "\n",
    "#First we will train our RandomForest model on the Bag-of-Words features and check its performance on both validation set and public leaderboard.\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain) \n",
    "prediction = rf.predict(xvalid_bow) \n",
    "# validation score \n",
    "f1_score(yvalid, prediction)\n",
    "###############\n",
    "test_pred = rf.predict(test_bow) \n",
    "test['label'] = test_pred \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_rf_bow.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.562152133580705"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain) \n",
    "prediction = rf.predict(xvalid_tfidf) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5021551724137931"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec Features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain) \n",
    "prediction = rf.predict(xvalid_w2v) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06223479490806223"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Doc2Vec Features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_d2v, ytrain) \n",
    "prediction = rf.predict(xvalid_d2v) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Bag-of-Words Features\n",
    "\n",
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain) \n",
    "prediction = xgb_model.predict(xvalid_bow) \n",
    "f1_score(yvalid, prediction)\n",
    "\n",
    "test_pred = xgb_model.predict(test_bow) \n",
    "test['label'] = test_pred \n",
    "submission = test[['id','label']] \n",
    "submission.to_csv('sub_xgb_bow.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5185891325071497"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Features\n",
    "\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain) \n",
    "prediction = xgb.predict(xvalid_tfidf) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6552655265526552"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec Features\n",
    "\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain) \n",
    "prediction = xgb.predict(xvalid_w2v) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doc2Vec Features\n",
    "\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_d2v, ytrain) \n",
    "prediction = xgb.predict(xvalid_d2v) \n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
